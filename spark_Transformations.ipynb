{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ea07ab14",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "import getpass\n",
    "\n",
    "username = getpass.getuser()\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Shubham-Malai\") \\\n",
    "    .master(\"yarn\") \\\n",
    "    .config(\"spark.ui.port\", \"0\") \\\n",
    "    .config(\"spark.sql.warehouse.dir\", f\"/user/{username}/warehouse\") \\\n",
    "    .enableHiveSupport() \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c01f68d0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - hive</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://g01.itversity.com:41915\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.1.2</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>yarn</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>Shubham-Malai</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7f2bf5d0b0b8>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "568fd32b",
   "metadata": {},
   "outputs": [],
   "source": [
    "list = [(\"Spring\", 12.3),\n",
    "(\"Summer\", 10.5),\n",
    "(\"Autumn\", 8.2),\n",
    "(\"Winter\", 15.1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b0ba20cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_schema = ['season','windspeed']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0142a4b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "weather_df = spark.createDataFrame(list, df_schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e64f3397",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+---------+\n",
      "|season|windspeed|\n",
      "+------+---------+\n",
      "|Spring|     12.3|\n",
      "|Summer|     10.5|\n",
      "|Autumn|      8.2|\n",
      "|Winter|     15.1|\n",
      "+------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "weather_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9f9e6186",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border='1'>\n",
       "<tr><th>summary</th><th>season</th><th>windspeed</th></tr>\n",
       "<tr><td>count</td><td>4</td><td>4</td></tr>\n",
       "<tr><td>mean</td><td>null</td><td>11.524999999999999</td></tr>\n",
       "<tr><td>stddev</td><td>null</td><td>2.9147612823923224</td></tr>\n",
       "<tr><td>min</td><td>Autumn</td><td>8.2</td></tr>\n",
       "<tr><td>max</td><td>Winter</td><td>15.1</td></tr>\n",
       "</table>\n"
      ],
      "text/plain": [
       "+-------+------+------------------+\n",
       "|summary|season|         windspeed|\n",
       "+-------+------+------------------+\n",
       "|  count|     4|                 4|\n",
       "|   mean|  null|11.524999999999999|\n",
       "| stddev|  null|2.9147612823923224|\n",
       "|    min|Autumn|               8.2|\n",
       "|    max|Winter|              15.1|\n",
       "+-------+------+------------------+"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weather_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d8dfbfef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- season: string (nullable = true)\n",
      " |-- windspeed: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "weather_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fe96b577",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f2c7d459",
   "metadata": {},
   "outputs": [],
   "source": [
    "library_schema = StructType([\n",
    "    StructField(\"books\",StringType()),\n",
    "    StructField(\"library_name\",StringType()),\n",
    "    StructField(\"location\",StringType()),\n",
    "    StructField(\"members\",StringType())\n",
    "\n",
    "    \n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "145ce08f",
   "metadata": {},
   "outputs": [],
   "source": [
    "library_df  = spark.read \\\n",
    ".format(\"json\") \\\n",
    ".schema(library_schema) \\\n",
    ".load(\"/public/trendytech/datasets/library_data.json\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0cc3ded7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----------------+-----------+--------------------+\n",
      "|               books|     library_name|   location|             members|\n",
      "+--------------------+-----------------+-----------+--------------------+\n",
      "|[{\"book_id\":\"B001...|  Central Library|City Center|[{\"member_id\":\"M0...|\n",
      "|[{\"book_id\":\"B003...|Community Library|     Suburb|[{\"member_id\":\"M0...|\n",
      "+--------------------+-----------------+-----------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "library_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b7a8b55c",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = spark.read \\\n",
    ".format(\"csv\") \\\n",
    ".option(\"header\",\"true\") \\\n",
    ".load(\"/public/trendytech/datasets/train.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "dba11d25",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+----------+---------------+--------------+---+-------------+-----------+\n",
      "|train_number|train_name|seats_available|passenger_name|age|ticket_number|seat_number|\n",
      "+------------+----------+---------------+--------------+---+-------------+-----------+\n",
      "|         123|   Express|            100|          John| 25|         T123|         A1|\n",
      "|         123|   Express|            100|          Emma| 30|         T124|         B2|\n",
      "|         456| Superfast|            150|       Michael| 35|         T125|         C3|\n",
      "|         456| Superfast|            150|        Sophia| 40|         T126|         D4|\n",
      "|         789|     Local|             50|       William| 28|         T127|         E5|\n",
      "|         789|     Local|             50|        Sophia| 32|         T128|         F6|\n",
      "|         789|     Local|             50|        Oliver| 45|         T129|         G7|\n",
      "+------------+----------+---------------+--------------+---+-------------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a1ebd06d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- train_number: string (nullable = true)\n",
      " |-- train_name: string (nullable = true)\n",
      " |-- seats_available: string (nullable = true)\n",
      " |-- passenger_name: string (nullable = true)\n",
      " |-- age: string (nullable = true)\n",
      " |-- ticket_number: string (nullable = true)\n",
      " |-- seat_number: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train_df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab57f685",
   "metadata": {},
   "source": [
    "a) Drop the columns passenger_name and age from the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9d253394",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df2 = train_df.drop(\"passenger_name\",\"age\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "11267dbd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+----------+---------------+-------------+-----------+\n",
      "|train_number|train_name|seats_available|ticket_number|seat_number|\n",
      "+------------+----------+---------------+-------------+-----------+\n",
      "|         123|   Express|            100|         T123|         A1|\n",
      "|         123|   Express|            100|         T124|         B2|\n",
      "|         456| Superfast|            150|         T125|         C3|\n",
      "|         456| Superfast|            150|         T126|         D4|\n",
      "|         789|     Local|             50|         T127|         E5|\n",
      "|         789|     Local|             50|         T128|         F6|\n",
      "|         789|     Local|             50|         T129|         G7|\n",
      "+------------+----------+---------------+-------------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train_df2.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79c18fbc",
   "metadata": {},
   "source": [
    "b) Count the number of rows after removing duplicates of columns\n",
    "train_number and ticket_number."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "efa9dc8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df3 = train_df2.dropDuplicates([\"train_number\",\"ticket_number\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a9dbf660",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+----------+---------------+-------------+-----------+\n",
      "|train_number|train_name|seats_available|ticket_number|seat_number|\n",
      "+------------+----------+---------------+-------------+-----------+\n",
      "|         789|     Local|             50|         T128|         F6|\n",
      "|         456| Superfast|            150|         T125|         C3|\n",
      "|         789|     Local|             50|         T129|         G7|\n",
      "|         123|   Express|            100|         T124|         B2|\n",
      "|         456| Superfast|            150|         T126|         D4|\n",
      "|         789|     Local|             50|         T127|         E5|\n",
      "|         123|   Express|            100|         T123|         A1|\n",
      "+------------+----------+---------------+-------------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train_df3.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c7e7df89",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df2.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ceece0f6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df3.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90d003d9",
   "metadata": {},
   "source": [
    "c) Count the number of unique train names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "38a8bb10",
   "metadata": {},
   "outputs": [],
   "source": [
    "distinct_trains = train_df3.select(\"train_name\").distinct()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "09f9fd60",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+\n",
      "|train_name|\n",
      "+----------+\n",
      "|   Express|\n",
      "|     Local|\n",
      "| Superfast|\n",
      "+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "distinct_trains.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcc3c53c",
   "metadata": {},
   "source": [
    "1. Read the dataset using the \"permissive\" mode and count the number of\n",
    "records read.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d704ecbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "sales_df  = spark.read \\\n",
    ".format(\"json\") \\\n",
    ".option(\"mode\",\"permissive\") \\\n",
    ".load(\"/public/trendytech/datasets/sales_data.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "5774b230",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+----------+--------+-------+--------+\n",
      "|     _corrupt_record|   product|quantity|revenue|store_id|\n",
      "+--------------------+----------+--------+-------+--------+\n",
      "|                null|     Apple|      10|  100.0|       1|\n",
      "|                null|    Banana|      15|   75.0|       2|\n",
      "|                null|    Orange|      12|   90.0|       3|\n",
      "|                null|     Mango|       8|  120.0|       4|\n",
      "|                null|     Grape|      20|  150.0|       5|\n",
      "|                null|Watermelon|       5|   50.0|       6|\n",
      "|                null|Strawberry|      18|  108.0|       7|\n",
      "|                null| Pineapple|      14|  140.0|       8|\n",
      "|                null|    Cherry|       7|  105.0|       9|\n",
      "|                null|      Pear|       9|   81.0|      10|\n",
      "|                null| Blueberry|      11|   88.0|      11|\n",
      "|                null|      Kiwi|      16|  128.0|      12|\n",
      "|                null|     Peach|      13|   91.0|      13|\n",
      "|                null|      Plum|       6|   54.0|      14|\n",
      "|                null|     Lemon|      10|   70.0|      15|\n",
      "|                null| Raspberry|      17|  136.0|      16|\n",
      "|                null|   Coconut|       4|   80.0|      17|\n",
      "|                null|   Avocado|      11|   99.0|      18|\n",
      "|                null|Blackberry|       8|   64.0|      19|\n",
      "|                null|         G| Invalid|    NaN|      20|\n",
      "|{\"store_id\": 21, ...|      null|    null|   null|    null|\n",
      "|                null|Watermelon|       5|Invalid|      22|\n",
      "+--------------------+----------+--------+-------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sales_df.show(22)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "e3915236",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "22"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sales_df.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d3612a0",
   "metadata": {},
   "source": [
    "Read the dataset using the \"dropmalformed\" mode and display the\n",
    "number of malformed records."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "98a749c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "sales_df2  = spark.read \\\n",
    ".format(\"json\") \\\n",
    ".option(\"mode\",\"dropmalformed\") \\\n",
    ".load(\"/public/trendytech/datasets/sales_data.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "a1f09b44",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "21"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sales_df2.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "9c45d9ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------+-------+--------+\n",
      "|   product|quantity|revenue|store_id|\n",
      "+----------+--------+-------+--------+\n",
      "|     Apple|      10|  100.0|       1|\n",
      "|    Banana|      15|   75.0|       2|\n",
      "|    Orange|      12|   90.0|       3|\n",
      "|     Mango|       8|  120.0|       4|\n",
      "|     Grape|      20|  150.0|       5|\n",
      "|Watermelon|       5|   50.0|       6|\n",
      "|Strawberry|      18|  108.0|       7|\n",
      "| Pineapple|      14|  140.0|       8|\n",
      "|    Cherry|       7|  105.0|       9|\n",
      "|      Pear|       9|   81.0|      10|\n",
      "| Blueberry|      11|   88.0|      11|\n",
      "|      Kiwi|      16|  128.0|      12|\n",
      "|     Peach|      13|   91.0|      13|\n",
      "|      Plum|       6|   54.0|      14|\n",
      "|     Lemon|      10|   70.0|      15|\n",
      "| Raspberry|      17|  136.0|      16|\n",
      "|   Coconut|       4|   80.0|      17|\n",
      "|   Avocado|      11|   99.0|      18|\n",
      "|Blackberry|       8|   64.0|      19|\n",
      "|         G| Invalid|    NaN|      20|\n",
      "|Watermelon|       5|Invalid|      22|\n",
      "+----------+--------+-------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sales_df2.show(21)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f007e86c",
   "metadata": {},
   "source": [
    "Read the dataset using the \"failfast\" mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "99e40f7b",
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o157.load.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 53.0 failed 4 times, most recent failure: Lost task 0.3 in stage 53.0 (TID 833) (w02.itversity.com executor 1): org.apache.spark.SparkException: Malformed records are detected in schema inference. Parse Mode: FAILFAST.\n\tat org.apache.spark.sql.catalyst.json.JsonInferSchema.$anonfun$infer$2(JsonInferSchema.scala:78)\n\tat scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:484)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:490)\n\tat scala.collection.Iterator.foreach(Iterator.scala:941)\n\tat scala.collection.Iterator.foreach$(Iterator.scala:941)\n\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1429)\n\tat scala.collection.TraversableOnce.reduceLeft(TraversableOnce.scala:190)\n\tat scala.collection.TraversableOnce.reduceLeft$(TraversableOnce.scala:183)\n\tat scala.collection.AbstractIterator.reduceLeft(Iterator.scala:1429)\n\tat scala.collection.TraversableOnce.reduceLeftOption(TraversableOnce.scala:208)\n\tat scala.collection.TraversableOnce.reduceLeftOption$(TraversableOnce.scala:207)\n\tat scala.collection.AbstractIterator.reduceLeftOption(Iterator.scala:1429)\n\tat scala.collection.TraversableOnce.reduceOption(TraversableOnce.scala:215)\n\tat scala.collection.TraversableOnce.reduceOption$(TraversableOnce.scala:215)\n\tat scala.collection.AbstractIterator.reduceOption(Iterator.scala:1429)\n\tat org.apache.spark.sql.catalyst.json.JsonInferSchema.$anonfun$infer$1(JsonInferSchema.scala:81)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2(RDD.scala:863)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2$adapted(RDD.scala:863)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:497)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:500)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: com.fasterxml.jackson.core.io.JsonEOFException: Unexpected end-of-input: expected close marker for Object (start marker at [Source: (byte[])\"{\"store_id\": 21, \"product\": \"Pineapple\", \"quantity\": 14, \"revenue\": 140.0\"; line: 1, column: 1])\n at [Source: (byte[])\"{\"store_id\": 21, \"product\": \"Pineapple\", \"quantity\": 14, \"revenue\": 140.0\"; line: 1, column: 220]\n\tat com.fasterxml.jackson.core.base.ParserMinimalBase._reportInvalidEOF(ParserMinimalBase.java:664)\n\tat com.fasterxml.jackson.core.base.ParserBase._handleEOF(ParserBase.java:486)\n\tat com.fasterxml.jackson.core.base.ParserBase._eofAsNextChar(ParserBase.java:498)\n\tat com.fasterxml.jackson.core.json.UTF8StreamJsonParser._skipWSOrEnd(UTF8StreamJsonParser.java:2957)\n\tat com.fasterxml.jackson.core.json.UTF8StreamJsonParser.nextToken(UTF8StreamJsonParser.java:715)\n\tat org.apache.spark.sql.catalyst.json.JacksonUtils$.nextUntil(JacksonUtils.scala:29)\n\tat org.apache.spark.sql.catalyst.json.JsonInferSchema.inferField(JsonInferSchema.scala:140)\n\tat org.apache.spark.sql.catalyst.json.JsonInferSchema.$anonfun$infer$4(JsonInferSchema.scala:68)\n\tat org.apache.spark.util.Utils$.tryWithResource(Utils.scala:2622)\n\tat org.apache.spark.sql.catalyst.json.JsonInferSchema.$anonfun$infer$2(JsonInferSchema.scala:66)\n\t... 28 more\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2258)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2207)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2206)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2206)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1079)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1079)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1079)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2445)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2387)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2376)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:868)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2196)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2291)\n\tat org.apache.spark.sql.catalyst.json.JsonInferSchema.infer(JsonInferSchema.scala:94)\n\tat org.apache.spark.sql.execution.datasources.json.TextInputJsonDataSource$.$anonfun$inferFromDataset$5(JsonDataSource.scala:110)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:163)\n\tat org.apache.spark.sql.execution.datasources.json.TextInputJsonDataSource$.inferFromDataset(JsonDataSource.scala:110)\n\tat org.apache.spark.sql.execution.datasources.json.TextInputJsonDataSource$.infer(JsonDataSource.scala:99)\n\tat org.apache.spark.sql.execution.datasources.json.JsonDataSource.inferSchema(JsonDataSource.scala:65)\n\tat org.apache.spark.sql.execution.datasources.json.JsonFileFormat.inferSchema(JsonFileFormat.scala:58)\n\tat org.apache.spark.sql.execution.datasources.DataSource.$anonfun$getOrInferFileFormatSchema$11(DataSource.scala:209)\n\tat scala.Option.orElse(Option.scala:447)\n\tat org.apache.spark.sql.execution.datasources.DataSource.getOrInferFileFormatSchema(DataSource.scala:206)\n\tat org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:419)\n\tat org.apache.spark.sql.DataFrameReader.loadV1Source(DataFrameReader.scala:325)\n\tat org.apache.spark.sql.DataFrameReader.$anonfun$load$3(DataFrameReader.scala:307)\n\tat scala.Option.getOrElse(Option.scala:189)\n\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:307)\n\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:239)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:750)\nCaused by: org.apache.spark.SparkException: Malformed records are detected in schema inference. Parse Mode: FAILFAST.\n\tat org.apache.spark.sql.catalyst.json.JsonInferSchema.$anonfun$infer$2(JsonInferSchema.scala:78)\n\tat scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:484)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:490)\n\tat scala.collection.Iterator.foreach(Iterator.scala:941)\n\tat scala.collection.Iterator.foreach$(Iterator.scala:941)\n\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1429)\n\tat scala.collection.TraversableOnce.reduceLeft(TraversableOnce.scala:190)\n\tat scala.collection.TraversableOnce.reduceLeft$(TraversableOnce.scala:183)\n\tat scala.collection.AbstractIterator.reduceLeft(Iterator.scala:1429)\n\tat scala.collection.TraversableOnce.reduceLeftOption(TraversableOnce.scala:208)\n\tat scala.collection.TraversableOnce.reduceLeftOption$(TraversableOnce.scala:207)\n\tat scala.collection.AbstractIterator.reduceLeftOption(Iterator.scala:1429)\n\tat scala.collection.TraversableOnce.reduceOption(TraversableOnce.scala:215)\n\tat scala.collection.TraversableOnce.reduceOption$(TraversableOnce.scala:215)\n\tat scala.collection.AbstractIterator.reduceOption(Iterator.scala:1429)\n\tat org.apache.spark.sql.catalyst.json.JsonInferSchema.$anonfun$infer$1(JsonInferSchema.scala:81)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2(RDD.scala:863)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2$adapted(RDD.scala:863)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:497)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:500)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: com.fasterxml.jackson.core.io.JsonEOFException: Unexpected end-of-input: expected close marker for Object (start marker at [Source: (byte[])\"{\"store_id\": 21, \"product\": \"Pineapple\", \"quantity\": 14, \"revenue\": 140.0\"; line: 1, column: 1])\n at [Source: UNKNOWN; line: 1, column: 220]\n\tat com.fasterxml.jackson.core.base.ParserMinimalBase._reportInvalidEOF(ParserMinimalBase.java:664)\n\tat com.fasterxml.jackson.core.base.ParserBase._handleEOF(ParserBase.java:486)\n\tat com.fasterxml.jackson.core.base.ParserBase._eofAsNextChar(ParserBase.java:498)\n\tat com.fasterxml.jackson.core.json.UTF8StreamJsonParser._skipWSOrEnd(UTF8StreamJsonParser.java:2957)\n\tat com.fasterxml.jackson.core.json.UTF8StreamJsonParser.nextToken(UTF8StreamJsonParser.java:715)\n\tat org.apache.spark.sql.catalyst.json.JacksonUtils$.nextUntil(JacksonUtils.scala:29)\n\tat org.apache.spark.sql.catalyst.json.JsonInferSchema.inferField(JsonInferSchema.scala:140)\n\tat org.apache.spark.sql.catalyst.json.JsonInferSchema.$anonfun$infer$4(JsonInferSchema.scala:68)\n\tat org.apache.spark.util.Utils$.tryWithResource(Utils.scala:2622)\n\tat org.apache.spark.sql.catalyst.json.JsonInferSchema.$anonfun$infer$2(JsonInferSchema.scala:66)\n\t... 28 more\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-46-3ddbcce1a882>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"json\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0moption\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"mode\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"failfast\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"/public/trendytech/datasets/sales_data.json\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/opt/spark-3.1.2-bin-hadoop3.2/python/pyspark/sql/readwriter.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(self, path, format, schema, **options)\u001b[0m\n\u001b[1;32m    202\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    203\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 204\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_df\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jreader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    205\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mpath\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    206\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/spark-3.1.2-bin-hadoop3.2/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1303\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1304\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1305\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1306\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1307\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/spark-3.1.2-bin-hadoop3.2/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    109\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 111\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    112\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m             \u001b[0mconverted\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconvert_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/spark-3.1.2-bin-hadoop3.2/python/lib/py4j-0.10.9-src.zip/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    326\u001b[0m                 raise Py4JJavaError(\n\u001b[1;32m    327\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 328\u001b[0;31m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[1;32m    329\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    330\u001b[0m                 raise Py4JError(\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o157.load.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 53.0 failed 4 times, most recent failure: Lost task 0.3 in stage 53.0 (TID 833) (w02.itversity.com executor 1): org.apache.spark.SparkException: Malformed records are detected in schema inference. Parse Mode: FAILFAST.\n\tat org.apache.spark.sql.catalyst.json.JsonInferSchema.$anonfun$infer$2(JsonInferSchema.scala:78)\n\tat scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:484)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:490)\n\tat scala.collection.Iterator.foreach(Iterator.scala:941)\n\tat scala.collection.Iterator.foreach$(Iterator.scala:941)\n\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1429)\n\tat scala.collection.TraversableOnce.reduceLeft(TraversableOnce.scala:190)\n\tat scala.collection.TraversableOnce.reduceLeft$(TraversableOnce.scala:183)\n\tat scala.collection.AbstractIterator.reduceLeft(Iterator.scala:1429)\n\tat scala.collection.TraversableOnce.reduceLeftOption(TraversableOnce.scala:208)\n\tat scala.collection.TraversableOnce.reduceLeftOption$(TraversableOnce.scala:207)\n\tat scala.collection.AbstractIterator.reduceLeftOption(Iterator.scala:1429)\n\tat scala.collection.TraversableOnce.reduceOption(TraversableOnce.scala:215)\n\tat scala.collection.TraversableOnce.reduceOption$(TraversableOnce.scala:215)\n\tat scala.collection.AbstractIterator.reduceOption(Iterator.scala:1429)\n\tat org.apache.spark.sql.catalyst.json.JsonInferSchema.$anonfun$infer$1(JsonInferSchema.scala:81)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2(RDD.scala:863)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2$adapted(RDD.scala:863)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:497)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:500)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: com.fasterxml.jackson.core.io.JsonEOFException: Unexpected end-of-input: expected close marker for Object (start marker at [Source: (byte[])\"{\"store_id\": 21, \"product\": \"Pineapple\", \"quantity\": 14, \"revenue\": 140.0\"; line: 1, column: 1])\n at [Source: (byte[])\"{\"store_id\": 21, \"product\": \"Pineapple\", \"quantity\": 14, \"revenue\": 140.0\"; line: 1, column: 220]\n\tat com.fasterxml.jackson.core.base.ParserMinimalBase._reportInvalidEOF(ParserMinimalBase.java:664)\n\tat com.fasterxml.jackson.core.base.ParserBase._handleEOF(ParserBase.java:486)\n\tat com.fasterxml.jackson.core.base.ParserBase._eofAsNextChar(ParserBase.java:498)\n\tat com.fasterxml.jackson.core.json.UTF8StreamJsonParser._skipWSOrEnd(UTF8StreamJsonParser.java:2957)\n\tat com.fasterxml.jackson.core.json.UTF8StreamJsonParser.nextToken(UTF8StreamJsonParser.java:715)\n\tat org.apache.spark.sql.catalyst.json.JacksonUtils$.nextUntil(JacksonUtils.scala:29)\n\tat org.apache.spark.sql.catalyst.json.JsonInferSchema.inferField(JsonInferSchema.scala:140)\n\tat org.apache.spark.sql.catalyst.json.JsonInferSchema.$anonfun$infer$4(JsonInferSchema.scala:68)\n\tat org.apache.spark.util.Utils$.tryWithResource(Utils.scala:2622)\n\tat org.apache.spark.sql.catalyst.json.JsonInferSchema.$anonfun$infer$2(JsonInferSchema.scala:66)\n\t... 28 more\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2258)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2207)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2206)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2206)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1079)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1079)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1079)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2445)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2387)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2376)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:868)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2196)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2291)\n\tat org.apache.spark.sql.catalyst.json.JsonInferSchema.infer(JsonInferSchema.scala:94)\n\tat org.apache.spark.sql.execution.datasources.json.TextInputJsonDataSource$.$anonfun$inferFromDataset$5(JsonDataSource.scala:110)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:163)\n\tat org.apache.spark.sql.execution.datasources.json.TextInputJsonDataSource$.inferFromDataset(JsonDataSource.scala:110)\n\tat org.apache.spark.sql.execution.datasources.json.TextInputJsonDataSource$.infer(JsonDataSource.scala:99)\n\tat org.apache.spark.sql.execution.datasources.json.JsonDataSource.inferSchema(JsonDataSource.scala:65)\n\tat org.apache.spark.sql.execution.datasources.json.JsonFileFormat.inferSchema(JsonFileFormat.scala:58)\n\tat org.apache.spark.sql.execution.datasources.DataSource.$anonfun$getOrInferFileFormatSchema$11(DataSource.scala:209)\n\tat scala.Option.orElse(Option.scala:447)\n\tat org.apache.spark.sql.execution.datasources.DataSource.getOrInferFileFormatSchema(DataSource.scala:206)\n\tat org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:419)\n\tat org.apache.spark.sql.DataFrameReader.loadV1Source(DataFrameReader.scala:325)\n\tat org.apache.spark.sql.DataFrameReader.$anonfun$load$3(DataFrameReader.scala:307)\n\tat scala.Option.getOrElse(Option.scala:189)\n\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:307)\n\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:239)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:750)\nCaused by: org.apache.spark.SparkException: Malformed records are detected in schema inference. Parse Mode: FAILFAST.\n\tat org.apache.spark.sql.catalyst.json.JsonInferSchema.$anonfun$infer$2(JsonInferSchema.scala:78)\n\tat scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:484)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:490)\n\tat scala.collection.Iterator.foreach(Iterator.scala:941)\n\tat scala.collection.Iterator.foreach$(Iterator.scala:941)\n\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1429)\n\tat scala.collection.TraversableOnce.reduceLeft(TraversableOnce.scala:190)\n\tat scala.collection.TraversableOnce.reduceLeft$(TraversableOnce.scala:183)\n\tat scala.collection.AbstractIterator.reduceLeft(Iterator.scala:1429)\n\tat scala.collection.TraversableOnce.reduceLeftOption(TraversableOnce.scala:208)\n\tat scala.collection.TraversableOnce.reduceLeftOption$(TraversableOnce.scala:207)\n\tat scala.collection.AbstractIterator.reduceLeftOption(Iterator.scala:1429)\n\tat scala.collection.TraversableOnce.reduceOption(TraversableOnce.scala:215)\n\tat scala.collection.TraversableOnce.reduceOption$(TraversableOnce.scala:215)\n\tat scala.collection.AbstractIterator.reduceOption(Iterator.scala:1429)\n\tat org.apache.spark.sql.catalyst.json.JsonInferSchema.$anonfun$infer$1(JsonInferSchema.scala:81)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2(RDD.scala:863)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2$adapted(RDD.scala:863)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:497)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:500)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: com.fasterxml.jackson.core.io.JsonEOFException: Unexpected end-of-input: expected close marker for Object (start marker at [Source: (byte[])\"{\"store_id\": 21, \"product\": \"Pineapple\", \"quantity\": 14, \"revenue\": 140.0\"; line: 1, column: 1])\n at [Source: UNKNOWN; line: 1, column: 220]\n\tat com.fasterxml.jackson.core.base.ParserMinimalBase._reportInvalidEOF(ParserMinimalBase.java:664)\n\tat com.fasterxml.jackson.core.base.ParserBase._handleEOF(ParserBase.java:486)\n\tat com.fasterxml.jackson.core.base.ParserBase._eofAsNextChar(ParserBase.java:498)\n\tat com.fasterxml.jackson.core.json.UTF8StreamJsonParser._skipWSOrEnd(UTF8StreamJsonParser.java:2957)\n\tat com.fasterxml.jackson.core.json.UTF8StreamJsonParser.nextToken(UTF8StreamJsonParser.java:715)\n\tat org.apache.spark.sql.catalyst.json.JacksonUtils$.nextUntil(JacksonUtils.scala:29)\n\tat org.apache.spark.sql.catalyst.json.JsonInferSchema.inferField(JsonInferSchema.scala:140)\n\tat org.apache.spark.sql.catalyst.json.JsonInferSchema.$anonfun$infer$4(JsonInferSchema.scala:68)\n\tat org.apache.spark.util.Utils$.tryWithResource(Utils.scala:2622)\n\tat org.apache.spark.sql.catalyst.json.JsonInferSchema.$anonfun$infer$2(JsonInferSchema.scala:66)\n\t... 28 more\n"
     ]
    }
   ],
   "source": [
    "sales_df3  = spark.read \\\n",
    ".format(\"json\") \\\n",
    ".option(\"mode\",\"failfast\") \\\n",
    ".load(\"/public/trendytech/datasets/sales_data.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "cc1c5736",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "efdaf9ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "hospital_df = spark.read \\\n",
    ".format(\"csv\") \\\n",
    ".option(\"header\",\"true\") \\\n",
    ".load(\"/public/trendytech/datasets/hospital.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "0f23cc7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------------+--------------+-------------+---------+----------+\n",
      "|patient_id|admission_date|discharge_date|    diagnosis|doctor_id|total_cost|\n",
      "+----------+--------------+--------------+-------------+---------+----------+\n",
      "|         1|    01-01-2022|    2022-01-10|    Pneumonia|      101|   5000.00|\n",
      "|         2|    02-05-2022|    2022-02-09| Appendicitis|      102|   7000.00|\n",
      "|         3|    03-12-2022|    2022-03-18|Fractured Arm|      103|   3500.00|\n",
      "|         4|    04-02-2022|    2022-04-08| Heart Attack|      104|  15000.00|\n",
      "|         5|    05-05-2022|    2022-05-07|    Influenza|      105|   2500.00|\n",
      "+----------+--------------+--------------+-------------+---------+----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "hospital_df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27d02d00",
   "metadata": {},
   "source": [
    "1. Drop the \"doctor_id\" column from the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "a3488d93",
   "metadata": {},
   "outputs": [],
   "source": [
    "hospital_df2 = hospital_df.drop(\"doctor_id\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "c49f2e82",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------------+--------------+-------------+----------+\n",
      "|patient_id|admission_date|discharge_date|    diagnosis|total_cost|\n",
      "+----------+--------------+--------------+-------------+----------+\n",
      "|         1|    01-01-2022|    2022-01-10|    Pneumonia|   5000.00|\n",
      "|         2|    02-05-2022|    2022-02-09| Appendicitis|   7000.00|\n",
      "|         3|    03-12-2022|    2022-03-18|Fractured Arm|   3500.00|\n",
      "|         4|    04-02-2022|    2022-04-08| Heart Attack|  15000.00|\n",
      "|         5|    05-05-2022|    2022-05-07|    Influenza|   2500.00|\n",
      "+----------+--------------+--------------+-------------+----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "hospital_df2.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ec2bbd2",
   "metadata": {},
   "source": [
    ". Rename the \"total_cost\" column to \"hospital_bill\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "8d03cf4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "hospital_df3 = hospital_df2.withColumnRenamed(\"total_cost\",\"hospital_bill\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "57c1e064",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------------+--------------+-------------+-------------+\n",
      "|patient_id|admission_date|discharge_date|    diagnosis|hospital_bill|\n",
      "+----------+--------------+--------------+-------------+-------------+\n",
      "|         1|    01-01-2022|    2022-01-10|    Pneumonia|      5000.00|\n",
      "|         2|    02-05-2022|    2022-02-09| Appendicitis|      7000.00|\n",
      "|         3|    03-12-2022|    2022-03-18|Fractured Arm|      3500.00|\n",
      "|         4|    04-02-2022|    2022-04-08| Heart Attack|     15000.00|\n",
      "|         5|    05-05-2022|    2022-05-07|    Influenza|      2500.00|\n",
      "+----------+--------------+--------------+-------------+-------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "hospital_df3.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "52234020",
   "metadata": {},
   "outputs": [],
   "source": [
    "hospital_df4 = hospital_df3.withColumn(\"admission_date\", date_format(to_date(\"admission_date\", \"MM-dd-yyyy\"),\n",
    "        \"yyyy-MM-dd\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d2243f8",
   "metadata": {},
   "source": [
    "hospital_df4.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08d2b108",
   "metadata": {},
   "source": [
    "3. Add a new column called \"duration_of_stay\" that represents the number\n",
    "of days a patient stayed in the hospital. (hint: The duration should be\n",
    "calculated as the difference between the \"discharge_date\" and\n",
    "\"admission_date\" columns.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "dcac16d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "hospital_df5 = hospital_df4.withColumn(\"duration_of_stay\", datediff(col(\"discharge_date\"), col(\"admission_date\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "7ac05f9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------------+--------------+-------------+-------------+----------------+\n",
      "|patient_id|admission_date|discharge_date|    diagnosis|hospital_bill|duration_of_stay|\n",
      "+----------+--------------+--------------+-------------+-------------+----------------+\n",
      "|         1|    2022-01-01|    2022-01-10|    Pneumonia|      5000.00|               9|\n",
      "|         2|    2022-02-05|    2022-02-09| Appendicitis|      7000.00|               4|\n",
      "|         3|    2022-03-12|    2022-03-18|Fractured Arm|      3500.00|               6|\n",
      "|         4|    2022-04-02|    2022-04-08| Heart Attack|     15000.00|               6|\n",
      "|         5|    2022-05-05|    2022-05-07|    Influenza|      2500.00|               2|\n",
      "+----------+--------------+--------------+-------------+-------------+----------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "hospital_df5.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7638db52",
   "metadata": {},
   "source": [
    "4. Create a new column called \"adjusted_total_cost\" that calculates the\n",
    "adjusted total cost based on the diagnosis as follows:\n",
    "If the diagnosis is \"Heart Attack\", multiply the hospital_bill by 1.5.\n",
    "If the diagnosis is \"Appendicitis\", multiply the hospital_bill by 1.2.\n",
    "For any other diagnosis, keep the hospital_bill as it is"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "89e87716",
   "metadata": {},
   "outputs": [],
   "source": [
    "hospital_df6 = hospital_df5.withColumn(\n",
    "    \"adjusted_total_cost\",\n",
    "    expr(\"\"\"\n",
    "        CASE\n",
    "            WHEN diagnosis = \"Heart Attack\" THEN hospital_bill*1.5\n",
    "            WHEN diagnosis = \"Appendicitis\" THEN hospital_bill*1.2\n",
    "            ELSE hospital_bill\n",
    "        END  \n",
    "        \"\"\")\n",
    "                                      )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "ed730ce4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------------+--------------+-------------+-------------+----------------+-------------------+\n",
      "|patient_id|admission_date|discharge_date|    diagnosis|hospital_bill|duration_of_stay|adjusted_total_cost|\n",
      "+----------+--------------+--------------+-------------+-------------+----------------+-------------------+\n",
      "|         1|    2022-01-01|    2022-01-10|    Pneumonia|      5000.00|               9|            5000.00|\n",
      "|         2|    2022-02-05|    2022-02-09| Appendicitis|      7000.00|               4|             8400.0|\n",
      "|         3|    2022-03-12|    2022-03-18|Fractured Arm|      3500.00|               6|            3500.00|\n",
      "|         4|    2022-04-02|    2022-04-08| Heart Attack|     15000.00|               6|            22500.0|\n",
      "|         5|    2022-05-05|    2022-05-07|    Influenza|      2500.00|               2|            2500.00|\n",
      "+----------+--------------+--------------+-------------+-------------+----------------+-------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "hospital_df6.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99c7e730",
   "metadata": {},
   "source": [
    "Select the \"patient_id\", \"diagnosis\", \"hospital_bill\", and\n",
    "\"adjusted_total_cost\" columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "d9cd1d13",
   "metadata": {},
   "outputs": [],
   "source": [
    "hospital_final_df = hospital_df6.select(\"patient_id\", \"diagnosis\", \"hospital_bill\",\n",
    "\"adjusted_total_cost\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "ee06b31b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------------+-------------+-------------------+\n",
      "|patient_id|    diagnosis|hospital_bill|adjusted_total_cost|\n",
      "+----------+-------------+-------------+-------------------+\n",
      "|         1|    Pneumonia|      5000.00|            5000.00|\n",
      "|         2| Appendicitis|      7000.00|             8400.0|\n",
      "|         3|Fractured Arm|      3500.00|            3500.00|\n",
      "|         4| Heart Attack|     15000.00|            22500.0|\n",
      "|         5|    Influenza|      2500.00|            2500.00|\n",
      "|         6| Appendicitis|      8000.00|             9600.0|\n",
      "|         7|    Pneumonia|      5500.00|            5500.00|\n",
      "|         8| Heart Attack|     20000.00|            30000.0|\n",
      "|         9|Fractured Leg|      6000.00|            6000.00|\n",
      "|        10| Appendicitis|      7500.00|             9000.0|\n",
      "|        11|    Influenza|      2800.00|            2800.00|\n",
      "|        12|    Pneumonia|      6000.00|            6000.00|\n",
      "|        13| Heart Attack|     18000.00|            27000.0|\n",
      "|        14| Appendicitis|      7200.00|             8640.0|\n",
      "|        15|Fractured Arm|      3800.00|            3800.00|\n",
      "|        16|    Influenza|      2700.00|            2700.00|\n",
      "|        17| Heart Attack|     16000.00|            24000.0|\n",
      "|        18|    Pneumonia|      4800.00|            4800.00|\n",
      "|        19|Fractured Leg|      6500.00|            6500.00|\n",
      "|        20| Appendicitis|      7800.00|             9360.0|\n",
      "+----------+-------------+-------------+-------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "hospital_final_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa2393e5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Pyspark 3",
   "language": "python",
   "name": "pyspark3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
